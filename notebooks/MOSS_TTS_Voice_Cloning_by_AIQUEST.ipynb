{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHECjALiQ396"
      },
      "source": [
        "# üéôÔ∏è MOSS-TTS 1.7B  (Zero-Shot Voice Cloning)\n",
        "### Free Colab Notebook (T4 GPU) | Made with ‚ù§Ô∏è by **AIQUEST**\n",
        "\n",
        "---\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "  <img src=\"https://img.shields.io/badge/AIQUESTAcademy-blueviolet?style=for-the-badge&logo=youtube&logoColor=white\" />\n",
        "  <img src=\"https://img.shields.io/badge/Colab-Free%20Tier-orange?style=for-the-badge&logo=googlecolab&logoColor=white\" />\n",
        "  <img src=\"https://img.shields.io/badge/Model-1.7B%20Params-green?style=for-the-badge\" />\n",
        "\n",
        "  <br><br>\n",
        "\n",
        "  <a href=\"https://www.youtube.com/@aiquestacademy?sub_confirmation=1\">\n",
        "    <img src=\"https://img.shields.io/badge/‚ñ∂%20Subscribe%20on%20YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white\" />\n",
        "  </a>\n",
        "  &nbsp;\n",
        "  <a href=\"https://x.com/aiquestacademy\">\n",
        "    <img src=\"https://img.shields.io/badge/Follow%20on%20ùïè-000000?style=for-the-badge&logo=x&logoColor=white\" />\n",
        "  </a>\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "### üìñ What is this Notebook?\n",
        "\n",
        "This notebook lets you run **MOSS-TTS 1.7B** ‚Äî a state-of-the-art **zero-shot text-to-speech** and **voice cloning** model ‚Äî entirely **free** on Google Colab's T4 GPU.\n",
        "\n",
        "**Key Features:**\n",
        "- üó£Ô∏è **Zero-Shot Voice Cloning** ‚Äî Clone any voice from a short audio reference\n",
        "- üéµ **High-Quality TTS** ‚Äî 1.7 billion parameter model for natural speech\n",
        "- ‚ö° **T4 Optimized** ‚Äî Runs on Colab free tier (no Pro needed!)\n",
        "- üéõÔ∏è **Quality Presets** ‚Äî From fast drafts (8 RVQ) to maximum quality (32 RVQ)\n",
        "- üéöÔ∏è **Full Control** ‚Äî Temperature, top-p, top-k, repetition penalty, speed\n",
        "- üåê **Gradio UI** ‚Äî Beautiful web interface with shareable public link\n",
        "\n",
        "### üöÄ How to Use\n",
        "1. Make sure **Runtime ‚Üí Change runtime type ‚Üí T4 GPU** is selected\n",
        "2. Run all cells in order (Runtime ‚Üí Run all)\n",
        "3. Wait for the Gradio link to appear (~5-8 min on first run)\n",
        "4. Open the public link and start generating speech!\n",
        "\n",
        "### üìå Credits\n",
        "- **Model:** [MOSS-TTS](https://github.com/OpenMOSS/MOSS-TTS) by OpenMOSS Team\n",
        "- **Notebook:** Optimized & packaged by **AIQUEST** ([@aiquestacademy](https://youtube.com/@aiquestacademy))\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTzElGi5Q39-"
      },
      "source": [
        "### üîç Step 1: Check GPU\n",
        "Let's verify that a **T4 GPU** is available. If you see \"Tesla T4\", you're good to go!\n",
        "If not, go to **Runtime ‚Üí Change runtime type ‚Üí T4 GPU**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63DZkNHPQ39_"
      },
      "source": [
        "!nvidia-smi\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-3Ow5Z-Q39_"
      },
      "source": [
        "### üì¶ Step 2: Install Dependencies\n",
        "Installing PyTorch (CUDA 11.8), Transformers, Gradio, and other required packages.\n",
        "This takes ~2-3 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sedwX-rZQ39_"
      },
      "source": [
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers accelerate librosa soundfile gradio\n",
        "!pip install -q einops omegaconf pyyaml scipy datasets sentencepiece protobuf\n",
        "print(\"‚úÖ  Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yjCSxt4Q3-A"
      },
      "source": [
        "### üì• Step 3: Clone MOSS-TTS Repository\n",
        "Cloning the official MOSS-TTS repo from GitHub.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3DQ-FPeQ3-A"
      },
      "source": [
        "!git clone https://github.com/OpenMOSS/MOSS-TTS.git\n",
        "%cd MOSS-TTS\n",
        "print(\"‚úÖ MOSS-TTS repository cloned\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQrAYaLoQ3-A"
      },
      "source": [
        "### ‚öôÔ∏è Step 4: Load Model & Launch Gradio Interface\n",
        "This cell loads the **MOSS-TTS 1.7B** model and launches a **Gradio** web UI.\n",
        "\n",
        "**First run downloads ~13GB** of model weights ‚Äî this takes ~5-8 minutes.\n",
        "After that, you'll get a **public Gradio link** you can share with anyone!\n",
        "\n",
        "> üí° **Tip:** Use the **Fast (8 RVQ)** preset for the longest audio on free tier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "solYY_fRQ3-A"
      },
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import AutoModel, AutoProcessor, GenerationConfig\n",
        "import gradio as gr\n",
        "import os\n",
        "from datetime import datetime\n",
        "import importlib.util\n",
        "import traceback\n",
        "import gc\n",
        "import time\n",
        "import atexit\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# Memory optimization settings for T4\n",
        "torch.backends.cuda.enable_cudnn_sdp(True)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "torch.backends.cuda.enable_math_sdp(True)\n",
        "\n",
        "class DelayGenerationConfig(GenerationConfig):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layers = kwargs.get(\"layers\", [{} for _ in range(32)])\n",
        "        self.do_samples = kwargs.get(\"do_samples\", None)\n",
        "        self.n_vq_for_inference = 32\n",
        "\n",
        "# Global variables\n",
        "model = None\n",
        "processor = None\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "def cleanup_model():\n",
        "    \"\"\"Unload model from GPU memory\"\"\"\n",
        "    global model, processor\n",
        "    if model is not None:\n",
        "        print(\"üßπ Cleaning up model from GPU...\")\n",
        "        del model\n",
        "        model = None\n",
        "    if processor is not None:\n",
        "        if hasattr(processor, 'audio_tokenizer'):\n",
        "            del processor.audio_tokenizer\n",
        "        del processor\n",
        "        processor = None\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(\"‚úÖ GPU memory cleared!\")\n",
        "\n",
        "atexit.register(cleanup_model)\n",
        "\n",
        "def resolve_attn_implementation() -> str:\n",
        "    if device == \"cuda\":\n",
        "        return \"sdpa\"\n",
        "    return \"eager\"\n",
        "\n",
        "def load_model():\n",
        "    \"\"\"Load model with optimized settings\"\"\"\n",
        "    global model, processor\n",
        "\n",
        "    if model is None:\n",
        "        print(\"üîÑ Loading MOSS-TTS (this takes ~5 min on first run)...\")\n",
        "\n",
        "        attn_implementation = resolve_attn_implementation()\n",
        "        print(f\"Using attention: {attn_implementation}\")\n",
        "\n",
        "        processor = AutoProcessor.from_pretrained(\n",
        "            \"OpenMOSS-Team/MOSS-TTS-Local-Transformer\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "\n",
        "        processor.audio_tokenizer = processor.audio_tokenizer.to(device)\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        model = AutoModel.from_pretrained(\n",
        "            \"OpenMOSS-Team/MOSS-TTS-Local-Transformer\",\n",
        "            trust_remote_code=True,\n",
        "            attn_implementation=attn_implementation,\n",
        "            torch_dtype=dtype,\n",
        "            low_cpu_mem_usage=True,\n",
        "        ).to(device)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            vram = torch.cuda.memory_allocated() / 1024**3\n",
        "            print(f\"‚úÖ Model loaded! VRAM: {vram:.2f}GB\")\n",
        "\n",
        "    return model, processor\n",
        "\n",
        "PRESETS = {\n",
        "    \"Fast (8 RVQ)\": {\n",
        "        \"n_vq\": 8,\n",
        "        \"text_temp\": 1.5,\n",
        "        \"audio_temp\": 0.95,\n",
        "        \"text_top_p\": 1.0,\n",
        "        \"audio_top_p\": 0.95,\n",
        "        \"text_top_k\": 50,\n",
        "        \"audio_top_k\": 50,\n",
        "        \"audio_rep_pen\": 1.1\n",
        "    },\n",
        "    \"Balanced (16 RVQ)\": {\n",
        "        \"n_vq\": 16,\n",
        "        \"text_temp\": 1.5,\n",
        "        \"audio_temp\": 0.95,\n",
        "        \"text_top_p\": 1.0,\n",
        "        \"audio_top_p\": 0.95,\n",
        "        \"text_top_k\": 50,\n",
        "        \"audio_top_k\": 50,\n",
        "        \"audio_rep_pen\": 1.1\n",
        "    },\n",
        "    \"High Quality (24 RVQ)\": {\n",
        "        \"n_vq\": 24,\n",
        "        \"text_temp\": 1.5,\n",
        "        \"audio_temp\": 0.95,\n",
        "        \"text_top_p\": 1.0,\n",
        "        \"audio_top_p\": 0.95,\n",
        "        \"text_top_k\": 50,\n",
        "        \"audio_top_k\": 50,\n",
        "        \"audio_rep_pen\": 1.1\n",
        "    },\n",
        "    \"Maximum (32 RVQ)\": {\n",
        "        \"n_vq\": 32,\n",
        "        \"text_temp\": 1.5,\n",
        "        \"audio_temp\": 0.95,\n",
        "        \"text_top_p\": 1.0,\n",
        "        \"audio_top_p\": 0.95,\n",
        "        \"text_top_k\": 50,\n",
        "        \"audio_top_k\": 50,\n",
        "        \"audio_rep_pen\": 1.1\n",
        "    }\n",
        "}\n",
        "\n",
        "def apply_preset(preset_name):\n",
        "    \"\"\"Return preset values\"\"\"\n",
        "    preset = PRESETS[preset_name]\n",
        "    return (\n",
        "        preset[\"n_vq\"],\n",
        "        preset[\"text_temp\"],\n",
        "        preset[\"text_top_p\"],\n",
        "        preset[\"text_top_k\"],\n",
        "        preset[\"audio_temp\"],\n",
        "        preset[\"audio_top_p\"],\n",
        "        preset[\"audio_top_k\"],\n",
        "        preset[\"audio_rep_pen\"]\n",
        "    )\n",
        "\n",
        "def generate_speech(\n",
        "    text,\n",
        "    reference_audio,\n",
        "    max_new_tokens,\n",
        "    speed,\n",
        "    text_temp,\n",
        "    text_top_p,\n",
        "    text_top_k,\n",
        "    audio_temp,\n",
        "    audio_top_p,\n",
        "    audio_top_k,\n",
        "    audio_repetition_penalty,\n",
        "    n_vq,\n",
        "    progress=gr.Progress()\n",
        "):\n",
        "    \"\"\"Generate TTS with memory-efficient long-form generation\"\"\"\n",
        "\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        return None, \"‚ö†Ô∏è Please enter text!\"\n",
        "\n",
        "    try:\n",
        "        os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "        progress(0, desc=\"Loading model...\")\n",
        "        model, processor = load_model()\n",
        "\n",
        "        text_length = len(text)\n",
        "        estimated_duration = max_new_tokens / 12.5\n",
        "\n",
        "        status = f\"üìù Text: {text_length:,} chars\\n\"\n",
        "        status += f\"üéØ Target: {max_new_tokens} tokens (~{estimated_duration/60:.1f} min)\\n\\n\"\n",
        "\n",
        "        yield None, status\n",
        "\n",
        "        # Build conversation\n",
        "        progress(0.1, desc=\"Processing...\")\n",
        "        if reference_audio is not None:\n",
        "            status += f\"üéôÔ∏è Voice cloning: {os.path.basename(reference_audio)}\\n\"\n",
        "            conversations = [[\n",
        "                processor.build_user_message(text=text, reference=[reference_audio])\n",
        "            ]]\n",
        "        else:\n",
        "            status += \"üéôÔ∏è Default voice\\n\"\n",
        "            conversations = [[\n",
        "                processor.build_user_message(text=text)\n",
        "            ]]\n",
        "\n",
        "        yield None, status\n",
        "\n",
        "        # Process input\n",
        "        batch = processor(conversations, mode=\"generation\")\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        # Fix temperature bug\n",
        "        if text_temp == 1.0:\n",
        "            text_temp = 1.001\n",
        "        if audio_temp == 1.0:\n",
        "            audio_temp = 1.001\n",
        "\n",
        "        # Generation config\n",
        "        generation_config = DelayGenerationConfig()\n",
        "        generation_config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "        generation_config.eos_token_id = 151653\n",
        "        generation_config.max_new_tokens = max_new_tokens\n",
        "        generation_config.use_cache = True\n",
        "        generation_config.do_sample = True\n",
        "        generation_config.num_beams = 1\n",
        "\n",
        "        generation_config.n_vq_for_inference = n_vq\n",
        "        generation_config.do_samples = [True] * (n_vq + 1)\n",
        "        generation_config.layers = [\n",
        "            {\n",
        "                \"repetition_penalty\": 1.0,\n",
        "                \"temperature\": text_temp,\n",
        "                \"top_p\": text_top_p,\n",
        "                \"top_k\": text_top_k\n",
        "            }\n",
        "        ] + [\n",
        "            {\n",
        "                \"repetition_penalty\": audio_repetition_penalty,\n",
        "                \"temperature\": audio_temp,\n",
        "                \"top_p\": audio_top_p,\n",
        "                \"top_k\": audio_top_k\n",
        "            }\n",
        "        ] * n_vq\n",
        "\n",
        "        # Clear cache\n",
        "        progress(0.2, desc=\"Clearing cache...\")\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        status += f\"\\nüéµ Generating...\\n\"\n",
        "        yield None, status\n",
        "\n",
        "        # Generate\n",
        "        start_time = time.time()\n",
        "        progress(0.3, desc=\"Generating...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                generation_config=generation_config\n",
        "            )\n",
        "\n",
        "        gen_time = time.time() - start_time\n",
        "\n",
        "        progress(0.85, desc=\"Decoding...\")\n",
        "        status += f\"‚úÖ Generated in {gen_time:.1f}s\\n\"\n",
        "        status += \"üîä Decoding...\\n\"\n",
        "        yield None, status\n",
        "\n",
        "        # Decode\n",
        "        decoded_messages = processor.decode(outputs)\n",
        "        audio = decoded_messages[0].audio_codes_list[0]\n",
        "\n",
        "        # Clear memory\n",
        "        if device == \"cuda\":\n",
        "            del outputs, input_ids, attention_mask, batch, decoded_messages\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.synchronize()\n",
        "            gc.collect()\n",
        "\n",
        "        # Speed\n",
        "        progress(0.94, desc=\"Speed adjust...\")\n",
        "        if speed != 1.0:\n",
        "            sample_rate = processor.model_config.sampling_rate\n",
        "            new_sample_rate = int(sample_rate * speed)\n",
        "            resampler = torchaudio.transforms.Resample(\n",
        "                orig_freq=sample_rate,\n",
        "                new_freq=new_sample_rate\n",
        "            )\n",
        "            audio_resampled = resampler(audio.unsqueeze(0)).squeeze(0)\n",
        "            resampler_back = torchaudio.transforms.Resample(\n",
        "                orig_freq=new_sample_rate,\n",
        "                new_freq=sample_rate\n",
        "            )\n",
        "            audio = resampler_back(audio_resampled.unsqueeze(0)).squeeze(0)\n",
        "\n",
        "        progress(0.97, desc=\"Saving...\")\n",
        "\n",
        "        # Save\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        output_path = f\"outputs/moss_tts_{timestamp}.wav\"\n",
        "        torchaudio.save(\n",
        "            output_path,\n",
        "            audio.unsqueeze(0),\n",
        "            processor.model_config.sampling_rate\n",
        "        )\n",
        "\n",
        "        duration = len(audio) / processor.model_config.sampling_rate\n",
        "        vram = torch.cuda.memory_allocated() / 1024**3 if device == \"cuda\" else 0\n",
        "        rtf = gen_time / duration if duration > 0 else 0\n",
        "\n",
        "        progress(1.0, desc=\"Done!\")\n",
        "\n",
        "        status += f\"\\nüéâ SUCCESS!\\n\"\n",
        "        status += f\"üìè Audio: {duration:.1f}s ({duration/60:.2f} min)\\n\"\n",
        "        status += f\"‚è±Ô∏è Generation: {gen_time:.1f}s ({gen_time/60:.1f} min)\\n\"\n",
        "        status += f\"üöÄ RTF: {rtf:.2f}x\\n\"\n",
        "        status += f\"üéöÔ∏è Speed: {speed}x\\n\"\n",
        "        status += f\"üìä VRAM: {vram:.2f}GB\\n\"\n",
        "        status += f\"üéõÔ∏è RVQ: {n_vq}/32\\n\"\n",
        "        status += f\"üíæ {output_path}\"\n",
        "\n",
        "        yield output_path, status\n",
        "\n",
        "    except torch.cuda.OutOfMemoryError as e:\n",
        "        error_msg = f\"‚ùå OUT OF MEMORY!\\n\\n\"\n",
        "        error_msg += f\"Tried: {max_new_tokens} tokens with {n_vq} RVQ\\n\\n\"\n",
        "        error_msg += f\"Solutions:\\n\"\n",
        "        error_msg += f\"1. Reduce Max Tokens\\n\"\n",
        "        error_msg += f\"2. Use Fast (8 RVQ) preset\\n\"\n",
        "        error_msg += f\"3. Click 'Clear GPU' and retry\\n\\n\"\n",
        "        error_msg += f\"T4 Limits:\\n\"\n",
        "        error_msg += f\"‚Ä¢ 8 RVQ: ~7200 tokens (12 min)\\n\"\n",
        "        error_msg += f\"‚Ä¢ 16 RVQ: ~4800 tokens (8 min)\\n\"\n",
        "        error_msg += f\"‚Ä¢ 24 RVQ: ~3000 tokens (5 min)\\n\"\n",
        "        error_msg += f\"‚Ä¢ 32 RVQ: ~2400 tokens (4 min)\"\n",
        "        yield None, error_msg\n",
        "    except Exception as e:\n",
        "        error_msg = f\"‚ùå Error: {str(e)}\\n\\n{traceback.format_exc()}\"\n",
        "        yield None, error_msg\n",
        "\n",
        "# =============================================\n",
        "# GRADIO INTERFACE WITH AIQUEST BRANDING\n",
        "# =============================================\n",
        "\n",
        "custom_css = \"\"\"\n",
        ".aiquest-header {\n",
        "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "    padding: 20px;\n",
        "    border-radius: 12px;\n",
        "    margin-bottom: 16px;\n",
        "    text-align: center;\n",
        "    color: white;\n",
        "}\n",
        ".aiquest-header h1 {\n",
        "    margin: 0 0 8px 0;\n",
        "    font-size: 1.8em;\n",
        "    color: white !important;\n",
        "}\n",
        ".aiquest-header p {\n",
        "    margin: 4px 0;\n",
        "    opacity: 0.95;\n",
        "    color: white !important;\n",
        "}\n",
        ".social-buttons {\n",
        "    display: flex;\n",
        "    justify-content: center;\n",
        "    gap: 12px;\n",
        "    margin-top: 12px;\n",
        "    flex-wrap: wrap;\n",
        "}\n",
        ".social-buttons a {\n",
        "    display: inline-flex;\n",
        "    align-items: center;\n",
        "    gap: 6px;\n",
        "    padding: 8px 18px;\n",
        "    border-radius: 8px;\n",
        "    text-decoration: none;\n",
        "    font-weight: 600;\n",
        "    font-size: 0.95em;\n",
        "    transition: transform 0.2s, box-shadow 0.2s;\n",
        "}\n",
        ".social-buttons a:hover {\n",
        "    transform: translateY(-2px);\n",
        "    box-shadow: 0 4px 12px rgba(0,0,0,0.3);\n",
        "}\n",
        ".yt-btn {\n",
        "    background: #FF0000;\n",
        "    color: white !important;\n",
        "}\n",
        ".x-btn {\n",
        "    background: #000000;\n",
        "    color: white !important;\n",
        "}\n",
        ".aiquest-footer {\n",
        "    text-align: center;\n",
        "    padding: 14px;\n",
        "    margin-top: 16px;\n",
        "    border-radius: 10px;\n",
        "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "    color: white;\n",
        "    font-size: 0.9em;\n",
        "}\n",
        ".aiquest-footer a {\n",
        "    color: #ffd700 !important;\n",
        "    text-decoration: none;\n",
        "    font-weight: 600;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(\n",
        "    title=\"MOSS-TTS by AIQUEST\",\n",
        "    theme=gr.themes.Soft(),\n",
        "    css=custom_css\n",
        ") as demo:\n",
        "\n",
        "    # ---- HEADER WITH BRANDING ----\n",
        "    gr.HTML(\"\"\"\n",
        "    <div class=\"aiquest-header\">\n",
        "        <h1>üéôÔ∏è MOSS-TTS 1.7B Zero-Shot Voice Cloning</h1>\n",
        "        <p>1.7B parameter TTS & voice cloning ‚Ä¢ Optimized for Colab Free Tier (T4 GPU)</p>\n",
        "        <p style=\"font-size:0.85em; opacity:0.8;\">Model by OpenMOSS ‚Ä¢ Notebook by <b>AIQUEST</b></p>\n",
        "        <div class=\"social-buttons\">\n",
        "            <a href=\"https://www.youtube.com/@aiquestacademy?sub_confirmation=1\" target=\"_blank\" class=\"yt-btn\">\n",
        "                ‚ñ∂ Subscribe on YouTube\n",
        "            </a>\n",
        "            <a href=\"https://x.com/aiquestacademy\" target=\"_blank\" class=\"x-btn\">\n",
        "                ùïè Follow on X\n",
        "            </a>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            text_input = gr.Textbox(\n",
        "                label=\"üìù Text\",\n",
        "                placeholder=\"Paste your script here...\",\n",
        "                lines=10,\n",
        "                value=\"Hello! This is MOSS text-to-speech, running on Google Colab free tier. Notebook by AIQUEST.\"\n",
        "            )\n",
        "\n",
        "            reference_audio = gr.Audio(\n",
        "                label=\"üé§ Reference Voice (Optional ‚Äî upload for voice cloning)\",\n",
        "                type=\"filepath\",\n",
        "                sources=[\"upload\"]\n",
        "            )\n",
        "\n",
        "            preset_dropdown = gr.Dropdown(\n",
        "                choices=list(PRESETS.keys()),\n",
        "                value=\"Balanced (16 RVQ)\",\n",
        "                label=\"Preset\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                max_tokens = gr.Slider(\n",
        "                    50, 5000, 2500, step=100,\n",
        "                    label=\"Max Tokens\"\n",
        "                )\n",
        "                speed = gr.Slider(\n",
        "                    0.5, 2.0, 1.0, step=0.1,\n",
        "                    label=\"Speed\"\n",
        "                )\n",
        "\n",
        "            with gr.Accordion(\"‚öôÔ∏è Advanced Settings\", open=False):\n",
        "                n_vq = gr.Slider(8, 32, 8, step=1, label=\"RVQ Layers\")\n",
        "                with gr.Row():\n",
        "                    text_temp = gr.Slider(0.1, 2.0, 1.5, step=0.1, label=\"Text Temp\")\n",
        "                    text_top_p = gr.Slider(0.1, 1.0, 1.0, step=0.05, label=\"Text Top-P\")\n",
        "                    text_top_k = gr.Slider(1, 100, 50, step=1, label=\"Text Top-K\")\n",
        "                with gr.Row():\n",
        "                    audio_temp = gr.Slider(0.1, 2.0, 0.95, step=0.05, label=\"Audio Temp\")\n",
        "                    audio_top_p = gr.Slider(0.1, 1.0, 0.95, step=0.05, label=\"Audio Top-P\")\n",
        "                with gr.Row():\n",
        "                    audio_top_k = gr.Slider(1, 100, 50, step=1, label=\"Audio Top-K\")\n",
        "                    audio_rep_pen = gr.Slider(1.0, 1.5, 1.1, step=0.05, label=\"Rep Penalty\")\n",
        "\n",
        "            with gr.Row():\n",
        "                generate_btn = gr.Button(\"üéµ Generate Speech\", variant=\"primary\", size=\"lg\", scale=3)\n",
        "                clear_btn = gr.Button(\"üßπ Clear GPU\", variant=\"secondary\", size=\"lg\", scale=1)\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            audio_output = gr.Audio(label=\"üîä Generated Audio\", type=\"filepath\")\n",
        "            status_output = gr.Textbox(label=\"üìä Status\", lines=16, interactive=False)\n",
        "\n",
        "    # ---- FOOTER WITH BRANDING ----\n",
        "    gr.HTML(\"\"\"\n",
        "    <div class=\"aiquest-footer\">\n",
        "        Made with ‚ù§Ô∏è by <b>AIQUEST</b> &nbsp;|&nbsp;\n",
        "        <a href=\"https://www.youtube.com/@aiquestacademy?sub_confirmation=1\" target=\"_blank\">‚ñ∂ YouTube</a> &nbsp;|&nbsp;\n",
        "        <a href=\"https://x.com/aiquestacademy\" target=\"_blank\">ùïè / Twitter</a>\n",
        "        <br><span style=\"opacity:0.7;\">If you found this useful, please subscribe & share! üôè</span>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    preset_dropdown.change(\n",
        "        fn=apply_preset,\n",
        "        inputs=[preset_dropdown],\n",
        "        outputs=[n_vq, text_temp, text_top_p, text_top_k,\n",
        "                audio_temp, audio_top_p, audio_top_k, audio_rep_pen]\n",
        "    )\n",
        "\n",
        "    generate_btn.click(\n",
        "        fn=generate_speech,\n",
        "        inputs=[text_input, reference_audio, max_tokens, speed,\n",
        "                text_temp, text_top_p, text_top_k,\n",
        "                audio_temp, audio_top_p, audio_top_k,\n",
        "                audio_rep_pen, n_vq],\n",
        "        outputs=[audio_output, status_output]\n",
        "    )\n",
        "\n",
        "    def clear_memory():\n",
        "        cleanup_model()\n",
        "        return \"‚úÖ GPU cleared! Ready for next generation.\"\n",
        "\n",
        "    clear_btn.click(fn=clear_memory, inputs=[], outputs=[status_output])\n",
        "\n",
        "print(\"‚úÖ MOSS-TTS ready! Launching Gradio...\")\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ECaq3tQ3-B"
      },
      "source": [
        "---\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "### üéâ Enjoyed this notebook?\n",
        "\n",
        "If this was helpful, please **‚≠ê star the repo** and **subscribe** for more free AI tools & tutorials!\n",
        "\n",
        "  <a href=\"https://www.youtube.com/@aiquestacademy?sub_confirmation=1\">\n",
        "    <img src=\"https://img.shields.io/badge/‚ñ∂%20Subscribe%20on%20YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white\" />\n",
        "  </a>\n",
        "  &nbsp;\n",
        "  <a href=\"https://x.com/aiquestacademy\">\n",
        "    <img src=\"https://img.shields.io/badge/Follow%20on%20ùïè-000000?style=for-the-badge&logo=x&logoColor=white\" />\n",
        "  </a>\n",
        "\n",
        "**Made with ‚ù§Ô∏è by AIQUEST** | [@aiquestacademy](https://youtube.com/@aiquestacademy)\n",
        "\n",
        "</div>\n"
      ]
    }
  ]
}